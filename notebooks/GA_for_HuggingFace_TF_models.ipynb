{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "DFcwxLnDJgvi",
   "metadata": {
    "id": "DFcwxLnDJgvi"
   },
   "source": [
    "# Using GA model wrapper with Hugging Face TF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5bf8d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5413,
     "status": "ok",
     "timestamp": 1675256805350,
     "user": {
      "displayName": "Robert S.",
      "userId": "15925788058961757968"
     },
     "user_tz": -60
    },
    "id": "4c5bf8d4",
    "outputId": "81831aa8-86aa-454e-c891-1b656eea89a8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "! pip install transformers datasets gradient-accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8388e",
   "metadata": {},
   "source": [
    "## How to use GA with HF TF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beab166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gradient_accumulator import GradientAccumulateModel\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#import your model\n",
    "from transformers import TFxxx\n",
    "\n",
    "#load your model checkpoint\n",
    "HF_model = TFxxx.from_pretrained(checkpoint)\n",
    "\n",
    "#Define your model inputs and outputs\n",
    "#The inputs are in most cases the output of your models tokenizer\n",
    "#e.g. test it with tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "#adapt the model_input below if needed\n",
    "\n",
    "input_ids = tf.keras.Input(shape=(None,), dtype='int32', name=\"input_ids\")\n",
    "attention_mask = tf.keras.Input(shape=(None,), dtype='int32', name=\"attention_mask\")\n",
    "\n",
    "model_input={'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "#create a new Model which has model.input and model.output properties\n",
    "new_model = Model(inputs=model_input, outputs=HF_model(model_input))\n",
    "\n",
    "#create the GA model\n",
    "model = GradientAccumulateModel(accum_steps=1, inputs=new_model.input, outputs=new_model.output)\n",
    "\n",
    "#delete the unnecessary models\n",
    "del HF_model, new_model\n",
    "\n",
    "#simply use this GradientAccumulateModel instead of the HF_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "STSUeMzCSC6j",
   "metadata": {
    "id": "STSUeMzCSC6j"
   },
   "source": [
    "## Example for TFEsmForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03JYV1KUJjQ6",
   "metadata": {
    "id": "03JYV1KUJjQ6"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from gradient_accumulator import GradientAccumulateModel\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, set_seed\n",
    "from datasets import Dataset\n",
    "\n",
    "#reset random state\n",
    "def set_seeds(s):\n",
    "    tf.random.set_seed(s)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)\n",
    "    set_seed(s)\n",
    "\n",
    "#load a model for sequence regression and the corresponding tokenizer\n",
    "def load_model(checkpoint, dropout):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1, classifier_dropout=dropout)\n",
    "\n",
    "    return tokenizer,model\n",
    "\n",
    "#get training inputs and labels\n",
    "def load_lists():\n",
    "    train=pd.read_pickle(\"./hf_example_data.pkl\")\n",
    "    #reduced size to run on CPU\n",
    "    train_sequences=train.iloc[:256,0].tolist() \n",
    "    train_labels=train.iloc[:256,1].tolist()\n",
    "\n",
    "    return train_sequences, train_labels\n",
    "\n",
    "#create a Hugging Face Dataset\n",
    "def create_dataset(tokenizer, seqs, labels):\n",
    "    #reduced sequence length to run on CPU\n",
    "    tokenized = tokenizer(seqs, max_length=64, padding=True, truncation=True) \n",
    "    dataset = Dataset.from_dict(tokenized)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "#create a tf dataset\n",
    "def tf_dataset(model, tokenizer, dataset, batch, seed, shuffle=True):\n",
    "    tf.random.set_seed(seed)     \n",
    "    tf_set = model.prepare_tf_dataset(\n",
    "      dataset,\n",
    "      batch_size=batch,\n",
    "      shuffle=shuffle,\n",
    "      tokenizer=tokenizer)\n",
    "\n",
    "    return tf_set\n",
    "\n",
    "#training function\n",
    "def run_experiment(checkpoint, dropout=0.2, batch=8, accum=1, epoch=10, lr=2e-5, seed=42, GA=True):\n",
    "\n",
    "    set_seeds(seed)\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    tokenizer, model = load_model(checkpoint,dropout)\n",
    "    model.summary()\n",
    "\n",
    "    train_sequences, train_labels = load_lists()\n",
    "    train_set=create_dataset(tokenizer, train_sequences, train_labels)\n",
    "    train_tf=tf_dataset(model, tokenizer, train_set, batch,seed)\n",
    "\n",
    "    if GA:\n",
    "        input_ids = tf.keras.Input(shape=(None,), dtype='int32', name=\"input_ids\")\n",
    "        attention_mask = tf.keras.Input(shape=(None,), dtype='int32', name=\"attention_mask\")\n",
    "        model_input={'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "        new_model = Model(inputs=model_input, outputs=model(model_input))\n",
    "        acum_model = GradientAccumulateModel(accum_steps=accum, inputs=new_model.input, outputs=new_model.output)\n",
    "\n",
    "        del model, new_model\n",
    "\n",
    "        model = acum_model\n",
    "    \n",
    "    #often transformer models are compiled without specifiying loss\n",
    "    #this does not work here!\n",
    "    #look up the model internal loss function and specifiy it here\n",
    "\n",
    "    #compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss=tf.keras.losses.MeanSquaredError()) \n",
    "\n",
    "    #train model\n",
    "    model.fit(train_tf, epochs=epoch)  \n",
    "\n",
    "    #save results if needed\n",
    "    \n",
    "    \n",
    "    #reset tf backend\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.compat.v1.reset_default_graph() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82_IVHsJ5VXy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176922,
     "status": "ok",
     "timestamp": 1675255539303,
     "user": {
      "displayName": "Robert S.",
      "userId": "15925788058961757968"
     },
     "user_tz": -60
    },
    "id": "82_IVHsJ5VXy",
    "outputId": "557aaaeb-fb8d-4aec-dfa9-e17b1de33fde",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing TFEsmForSequenceClassification: ['lm_head', 'esm/contact_head/regression/kernel:0', 'esm/contact_head/regression/bias:0']\n",
      "- This IS expected if you are initializing TFEsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFEsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFEsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_esm_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " esm (TFEsmMainLayer)        multiple                  7409008   \n",
      "                                                                 \n",
      " classifier (TFEsmClassifica  multiple                 103041    \n",
      " tionHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,512,049\n",
      "Trainable params: 7,512,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "32/32 [==============================] - 12s 142ms/step - loss: 1.9291\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 4s 140ms/step - loss: 1.3234\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 5s 155ms/step - loss: 1.2813\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 5s 156ms/step - loss: 1.2715\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 4s 140ms/step - loss: 1.2616\n"
     ]
    }
   ],
   "source": [
    "#Train ESM2 model without GradientAccumulateModel as Baseline\n",
    "run_experiment(\"facebook/esm2_t6_8M_UR50D\", batch=8, accum=1, epoch=5, lr=2e-5, GA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3xgXchtz97Ss",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187489,
     "status": "ok",
     "timestamp": 1675255331727,
     "user": {
      "displayName": "Robert S.",
      "userId": "15925788058961757968"
     },
     "user_tz": -60
    },
    "id": "3xgXchtz97Ss",
    "outputId": "77d01aa5-9628-47c5-fc2e-b7d49e738658",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing TFEsmForSequenceClassification: ['lm_head', 'esm/contact_head/regression/kernel:0', 'esm/contact_head/regression/bias:0']\n",
      "- This IS expected if you are initializing TFEsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFEsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFEsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_esm_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " esm (TFEsmMainLayer)        multiple                  7409008   \n",
      "                                                                 \n",
      " classifier (TFEsmClassifica  multiple                 103041    \n",
      " tionHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,512,049\n",
      "Trainable params: 7,512,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "32/32 [==============================] - 12s 149ms/step - loss: 1.9291\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 5s 169ms/step - loss: 1.3234\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 7s 214ms/step - loss: 1.2813\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 5s 170ms/step - loss: 1.2714\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 6s 178ms/step - loss: 1.2616\n"
     ]
    }
   ],
   "source": [
    "#Train ESM2 model with GradientAccumulateModel\n",
    "run_experiment(\"facebook/esm2_t6_8M_UR50D\", batch=8, accum=1, epoch=5, lr=2e-5, GA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Fvrx1X585tJh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205617,
     "status": "ok",
     "timestamp": 1675254350502,
     "user": {
      "displayName": "Robert S.",
      "userId": "15925788058961757968"
     },
     "user_tz": -60
    },
    "id": "Fvrx1X585tJh",
    "outputId": "00429bd9-e552-47d3-ff41-aaf6e10de82a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing TFEsmForSequenceClassification: ['lm_head', 'esm/contact_head/regression/kernel:0', 'esm/contact_head/regression/bias:0']\n",
      "- This IS expected if you are initializing TFEsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFEsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFEsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_esm_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " esm (TFEsmMainLayer)        multiple                  7409008   \n",
      "                                                                 \n",
      " classifier (TFEsmClassifica  multiple                 103041    \n",
      " tionHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,512,049\n",
      "Trainable params: 7,512,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "64/64 [==============================] - 12s 94ms/step - loss: 1.9291\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 7s 108ms/step - loss: 1.3234\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 6s 99ms/step - loss: 1.2813\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 8s 128ms/step - loss: 1.2715\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 7s 109ms/step - loss: 1.2616\n"
     ]
    }
   ],
   "source": [
    "#Train equivalent ESM2 model with GradientAccumulateModel batch size 4\n",
    "run_experiment(\"facebook/esm2_t6_8M_UR50D\", batch=4, accum=2, epoch=5, lr=2e-5, GA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "_v3-ZM9o5xf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221574,
     "status": "ok",
     "timestamp": 1675254604017,
     "user": {
      "displayName": "Robert S.",
      "userId": "15925788058961757968"
     },
     "user_tz": -60
    },
    "id": "_v3-ZM9o5xf5",
    "outputId": "2285559f-ea0e-49ce-fbb5-a6b9138db4e8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing TFEsmForSequenceClassification: ['lm_head', 'esm/contact_head/regression/kernel:0', 'esm/contact_head/regression/bias:0']\n",
      "- This IS expected if you are initializing TFEsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFEsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFEsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_esm_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " esm (TFEsmMainLayer)        multiple                  7409008   \n",
      "                                                                 \n",
      " classifier (TFEsmClassifica  multiple                 103041    \n",
      " tionHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,512,049\n",
      "Trainable params: 7,512,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "128/128 [==============================] - 20s 100ms/step - loss: 1.9291\n",
      "Epoch 2/5\n",
      "128/128 [==============================] - 12s 92ms/step - loss: 1.3234\n",
      "Epoch 3/5\n",
      "128/128 [==============================] - 12s 96ms/step - loss: 1.2813\n",
      "Epoch 4/5\n",
      "128/128 [==============================] - 14s 108ms/step - loss: 1.2715\n",
      "Epoch 5/5\n",
      "128/128 [==============================] - 14s 107ms/step - loss: 1.2616\n"
     ]
    }
   ],
   "source": [
    "#Train equivalent ESM2 model with GradientAccumulateModel batch size 2\n",
    "run_experiment(\"facebook/esm2_t6_8M_UR50D\", batch=2, accum=4, epoch=5, lr=2e-5, GA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdf6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DFcwxLnDJgvi",
    "STSUeMzCSC6j",
    "swoEEI7AQwZ3",
    "X4fgWi7-Qyrf",
    "b5M-TeEq81yj",
    "OvzSe1XqR85E",
    "945-5fkfC-Lx",
    "ZGVq8j9NDARE",
    "hy6-yeqk3u8c"
   ],
   "provenance": [
    {
     "file_id": "https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb",
     "timestamp": 1670229986129
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
